{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "086ebcac-c7a1-451c-8690-8f79b8d478f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-15T12:03:05.884308Z",
     "iopub.status.busy": "2024-08-15T12:03:05.883915Z",
     "iopub.status.idle": "2024-08-15T12:03:05.905728Z",
     "shell.execute_reply": "2024-08-15T12:03:05.905087Z",
     "shell.execute_reply.started": "2024-08-15T12:03:05.884289Z"
    }
   },
   "source": [
    "# **Segmentos del script principal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd6e6da-a966-4f20-b70d-f2067f77c0a0",
   "metadata": {},
   "source": [
    "### Configuración de parametros y spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba801754-74b0-433a-a3ce-1ac584ced768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T13:07:30.048113Z",
     "iopub.status.busy": "2024-08-16T13:07:30.047788Z",
     "iopub.status.idle": "2024-08-16T13:07:40.775922Z",
     "shell.execute_reply": "2024-08-16T13:07:40.775161Z",
     "shell.execute_reply.started": "2024-08-16T13:07:30.048096Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-16 13:07:40,631 - FromScript - INFO - Starting spark configuration...\n",
      "2024-08-16 13:07:40,631 - FromScript - INFO - Starting spark configuration...\n",
      "2024-08-16 13:07:40,666 - FromScript - INFO - Ended spark configuration..."
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"FromScript\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logger.info(\"Starting spark configuration...\")\n",
    "\n",
    "# ###############\n",
    "# # SPARK CONFS #\n",
    "# ###############\n",
    "logger.info(\"Starting spark configuration...\")\n",
    "\n",
    "conf = SparkConf()\n",
    "\n",
    "conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "conf.set(\"hive.metastore.client.factory.class\", \"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\")\n",
    "conf.set(\"hive.metastore.schema.verification\", \"false\")\n",
    "conf.set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "conf.set(\"spark.sql.legacy.parquet.datetimeRebaseModeInRead\", \"LEGACY\")\n",
    "\n",
    "# S3A Configuration\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.timeout\", \"30000\")  # 30 segundos\n",
    "conf.set(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"30000\")  # 30 segundos\n",
    "conf.set(\"spark.hadoop.fs.s3a.socket.timeout\", \"30000\")  # 30 segundos\n",
    "conf.set(\"spark.hadoop.fs.s3a.attempts.maximum\", \"10\")  # Intentar 10 veces\n",
    "conf.set(\"spark.hadoop.fs.s3a.retry.limit\", \"10\")  # Límite de reintentos\n",
    "conf.set(\"spark.hadoop.fs.s3a.retry.interval\", \"5000\")  # Intervalo de reintento de 5 segundos\n",
    "conf.set(\"spark.hadoop.fs.s3a.retry.throttle.delay\", \"5000\")  # Retraso de aceleración de reintento de 5 segundos\n",
    "\n",
    "# Aumentar timeout de broadcast y ajustar broadcast join threshold\n",
    "conf.set(\"spark.sql.broadcastTimeout\", \"600\")  # Aumentar timeout de broadcast a 600 segundos\n",
    "conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")  # Deshabilitar broadcast join\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"SparkTest\")\n",
    "    .config(conf=conf)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")\n",
    "\n",
    "# logger.info(\"Spark confs: %s\", str(sc.getConf().getAll()))\n",
    "logger.info(\"Ended spark configuration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00377c5-6a0f-49ed-97a2-c354a35bea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################\n",
    "# ## ARGS FROM ENVs HOST ##\n",
    "# #########################\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Query inputs and outputs\")\n",
    "# parser.add_argument(\"--source-bucket\", type=str)\n",
    "# parser.add_argument(\"--artifacts-bucket\", type=str)\n",
    "# parser.add_argument(\"--feature-group-name\", type=str)\n",
    "# parser.add_argument(\"--date\", type=str)\n",
    "# parser.add_argument(\"--dominio\", type=str)\n",
    "# parser.add_argument(\"--database\", type=str)\n",
    "# parser.add_argument(\"--previous-months\", type=str)\n",
    "# parser.add_argument('--record-event-time-name', type=str, default='EventTime')\n",
    "# args, _ = parser.parse_known_args()\n",
    "# logger.info(\"Arguments: \")\n",
    "# logger.info(args)\n",
    "\n",
    "SOURCE_BUCKET = 'ibk-fs-source-data-173811917907'\n",
    "ARTIFACTS_BUCKET = 'ibk-fs-artifacts-173811917907'\n",
    "FEATURE_GROUP_NAME = 'ibk-fs-group-fg1-domain4-mes'\n",
    "DATE_STR = '20240601'\n",
    "DOMINIO = 'domain4'\n",
    "DATABASE = 'e_perm_aws'\n",
    "NUM_PREVIOUS_MONTHS = '2'\n",
    "RECORD_EVENT_TIME_NAME = 'cod_mes_timestamp'\n",
    "\n",
    "# ################\n",
    "# # USER PARAMAS #\n",
    "# ################\n",
    "\n",
    "df_sample_value = 0.3  # TESTING\n",
    "num_previous_months = \"2\"\n",
    "partition_name = 'cod_mes'\n",
    "prefix_querys = f'{DOMINIO}/{FEATURE_GROUP_NAME}/features'\n",
    "prefix_data = f'{DOMINIO}/{FEATURE_GROUP_NAME}/data'\n",
    "tmp_prefix_data = f'{DOMINIO}/{FEATURE_GROUP_NAME}/tmp_data'\n",
    "path = f\"s3://{SOURCE_BUCKET}/{prefix_data}\"\n",
    "\n",
    "# Querys keys s3\n",
    "query1 = f'code/{prefix_querys}/sql/query.sql'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d46e4-040d-45ca-849b-1b059d32aa31",
   "metadata": {},
   "source": [
    "### Definición de funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62568cde-56cd-4b88-a190-fe7d42af7d5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T12:57:38.067583Z",
     "iopub.status.busy": "2024-08-16T12:57:38.067116Z",
     "iopub.status.idle": "2024-08-16T12:57:38.090271Z",
     "shell.execute_reply": "2024-08-16T12:57:38.089537Z",
     "shell.execute_reply.started": "2024-08-16T12:57:38.067563Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ####################\n",
    "# ## Querys Process ##\n",
    "# ####################\n",
    "# ADD YOUR QUERYS HERE, ONE FUNCTION FOR EACH QUERY WITH THE SAME STRUCTURE\n",
    "# BUT DIFFERENT QUERY PARAMS IF NEEDED\n",
    "\n",
    "def query_df(bucket_artifact, key_artifact, database, codmes):\n",
    "    \n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.get_object(Bucket=bucket_artifact, Key=key_artifact)\n",
    "    query = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    query = query.format(\n",
    "        database=database,\n",
    "        codmes=codmes\n",
    "    )\n",
    "\n",
    "    return query\n",
    "\n",
    "# ####################\n",
    "# ## Params Process ##\n",
    "# ####################\n",
    "\n",
    "def parameters_definition(input_date, input_previous_months):\n",
    "    \"\"\"\n",
    "    Metodo parameters_definition:\n",
    "    Metodo para obtener los parametros\n",
    "    requeridos para consulta SQL\n",
    "    \"\"\"\n",
    "    date_formatted = datetime.strptime(input_date, \"%Y%m%d\").date()\n",
    "    codmes = str(date_formatted.year)+str(date_formatted.month).zfill(2)\n",
    "    calendar_date = date_formatted.replace(day=1)\n",
    "    input_previous_months = int(input_previous_months)\n",
    "    month_dict = {}\n",
    "    dates_dict = {}\n",
    "    for i in range(0, input_previous_months+1):\n",
    "        key = f'lcodmesm{str(i).zfill(2)}'\n",
    "        value = calendar_date-relativedelta(months=i)\n",
    "        dates_dict.update({key: value})\n",
    "        value = str(value.year)+str(value.month).zfill(2)\n",
    "        month_dict.update({key: value})\n",
    "    codmes_previous = month_dict[f'lcodmesm{str(input_previous_months).zfill(2)}']\n",
    "    return codmes, month_dict, dates_dict, calendar_date, codmes_previous\n",
    "\n",
    "# #####################################\n",
    "# ## Estimador de tamaño de archivos ##\n",
    "# #####################################\n",
    "\n",
    "def estimator_size(df, bucket, prefix, partition):\n",
    "    s3_client = boto3.client('s3')\n",
    "    prefix = prefix.rstrip('/') + '/'\n",
    "\n",
    "    sample_df = df.sample(fraction=0.01)\n",
    "\n",
    "    tmp_sample_path = f\"s3://{bucket}/{prefix}\"\n",
    "    sample_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .partitionBy(partition) \\\n",
    "        .save(tmp_sample_path)\n",
    "\n",
    "    sample_df.unpersist()\n",
    "\n",
    "    # ## Obtener la partición de la muestra ###\n",
    "\n",
    "    partitions = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix, Delimiter='/')\n",
    "    partition_prefixes = [content['Prefix'] for content in partitions.get('CommonPrefixes', [])]\n",
    "    partition_prefix = partition_prefixes[0]\n",
    "\n",
    "    def list_objects(bucket, prefix):\n",
    "        total_size = 0\n",
    "        s3_objects = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        if 'Contents' in s3_objects:\n",
    "            total_size = sum(obj['Size'] for obj in s3_objects['Contents'])\n",
    "        return total_size\n",
    "\n",
    "    # Calcular el tamaño total de todos los archivos en la partición seleccionada\n",
    "    sample_dir_size = list_objects(bucket, partition_prefix)\n",
    "    total_dir_size_estimated = sample_dir_size * 100  # ya que la muestra es del 1%\n",
    "\n",
    "    desired_file_size = 150 * 1024 * 1024  # 150 MB\n",
    "    num_files = math.ceil(total_dir_size_estimated / desired_file_size)\n",
    "\n",
    "    s3_objects = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "    delete_keys = [{'Key': obj['Key']} for obj in s3_objects.get('Contents', [])]\n",
    "\n",
    "    if prefix in delete_keys[0]['Key']:\n",
    "        s3_client.delete_objects(Bucket=bucket, Delete={'Objects': delete_keys})\n",
    "\n",
    "    return num_files\n",
    "\n",
    "# ######################\n",
    "# ## Spark Transforms ##\n",
    "# ######################\n",
    "\n",
    "def transformations(df, input_record_event_time_name):\n",
    "    \"\"\"\n",
    "    Metodo transformations:\n",
    "    Transformaciones a las columnas y\n",
    "    tipos de datos\n",
    "    \"\"\"\n",
    "    df = df \\\n",
    "        .select(\n",
    "            col(\"cod_mes\").cast(IntegerType()),\n",
    "            col(\"tip_doc\").cast(IntegerType()),\n",
    "            col(\"key_value\").cast(StringType()),\n",
    "            col(\"feature1\").cast(StringType()),\n",
    "            # col(\"feature2\").cast(StringType()),\n",
    "            # col(\"feature3\").cast(StringType())\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "# ESCHEMA .JSON DEFINITION\n",
    "# {\n",
    "#     \"cod_mes\": \"Integral\",\n",
    "#     \"tip_doc\": \"Integral\",\n",
    "#     \"key_value\": \"String\",\n",
    "#     \"feature1\": \"String\",\n",
    "#     \"feature2\": \"String\",\n",
    "#     \"feature3\": \"String\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e7c17-889d-4de9-89b3-3c2a09502f2a",
   "metadata": {},
   "source": [
    "### Inicialización del proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a59260f-0cd9-4d4c-a488-2c563aec31a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T12:57:39.352921Z",
     "iopub.status.busy": "2024-08-16T12:57:39.352473Z",
     "iopub.status.idle": "2024-08-16T12:57:40.586224Z",
     "shell.execute_reply": "2024-08-16T12:57:40.585426Z",
     "shell.execute_reply.started": "2024-08-16T12:57:39.352901Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ################\n",
    "# ## INIT PRAMS ##\n",
    "# ################\n",
    "codmes, month_dict, dates_dict, calendar_date, codmes_previous = parameters_definition(DATE_STR, num_previous_months)\n",
    "\n",
    "\n",
    "# ########################\n",
    "# ## LOAD QUERY FROM S3 ##\n",
    "# ########################\n",
    "query = query_df(ARTIFACTS_BUCKET, query1, DATABASE, codmes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "408011dd-b5c9-43fd-b781-3d29254345b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T12:57:45.891082Z",
     "iopub.status.busy": "2024-08-16T12:57:45.890770Z",
     "iopub.status.idle": "2024-08-16T12:57:45.911927Z",
     "shell.execute_reply": "2024-08-16T12:57:45.911438Z",
     "shell.execute_reply.started": "2024-08-16T12:57:45.891059Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with t_tiempo as (\n",
      "    select codmes, fecdia\n",
      "        from (select DATE_FORMAT(TO_DATE(calendar_date, 'yyyy-MM-dd'),'yyyyMM') codmes,\n",
      "                    TO_DATE(calendar_date, 'yyyy-MM-dd') fecdia,\n",
      "                    TO_DATE(calendar_date, 'yyyy-MM-dd') calendar_date,\n",
      "                    Row_Number() Over(ORDER BY calendar_date DESC) rnk\n",
      "                from e_perm_aws.t_calendar_summary\n",
      "                WHERE calendar_date <= DATE_FORMAT(TO_DATE('202406','yyyyMM'), 'yyyy-MM-dd')\n",
      "            ) as A\n",
      "        where A.rnk = 1\n",
      "    ),\n",
      "    t_cliente as (\n",
      "        select p_codmes as cod_mes, key_value,ingreso_bruto as ingreso_bruto,cod_tipo_documento as tip_doc\n",
      "        from e_perm_aws.t_aws_360_cliente_mes_d3\n",
      "        where cast (p_codmes as int) = 202406\n",
      "    )\n",
      "\n",
      "\tselect t.cod_mes, t.key_value,t.ingreso_bruto,t.tip_doc\n",
      "\tfrom t_cliente as t"
     ]
    }
   ],
   "source": [
    "for line in query.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f884ed1c-35f5-4b02-a0c2-c87d061cbca5",
   "metadata": {},
   "source": [
    "--------------\n",
    "### Ejecucion SQL con spark\n",
    "\n",
    "La ejecución perezosa en PySpark significa que PySpark no ejecuta las operaciones inmediatamente cuando las definís en tu código. En lugar de eso, construye un plan de ejecución y espera hasta que realmente necesite producir un resultado. Esto permite optimizar el proceso antes de ejecutar cualquier operación.\n",
    "\n",
    "\n",
    "### ¿Cómo funciona?\n",
    "1. **Definición de Transformaciones**: Cuando escribís transformaciones en PySpark (como `filter`, `map`, `select`), estas no se ejecutan inmediatamente. PySpark simplemente agrega estas operaciones a un plan de ejecución.\n",
    "2. **Ejecución Diferida**: PySpark no realiza ninguna acción hasta que encuentra una operación que necesite un resultado concreto, como escribir datos a un archivo o mostrar datos.\n",
    "\n",
    "### Métodos comunes que \"rompen\" la ejecución perezosa (Acciones):\n",
    "1. **`show()`**: Muestra algunas filas del DataFrame en la consola.\n",
    "2. **`collect()`**: Recoge los datos y los trae al programa como una lista en Python.\n",
    "3. **`count()`**: Cuenta el número de filas del DataFrame.\n",
    "4. **`write()`**: Guarda el DataFrame en un archivo (por ejemplo, en formato Parquet).\n",
    "\n",
    "Estos métodos fuerzan a PySpark a ejecutar todas las transformaciones que definiste previamente para poder generar el resultado final.\n",
    "|\n",
    "**Ejemplo**: Si hacés varias transformaciones sobre un DataFrame y terminás con un `df.show()`, PySpark recién en ese momento ejecutará todas las transformaciones en cadena y mostrará el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c5782f-5197-48f9-bb3b-80838de54aa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T12:57:52.371794Z",
     "iopub.status.busy": "2024-08-16T12:57:52.371374Z",
     "iopub.status.idle": "2024-08-16T12:58:03.640517Z",
     "shell.execute_reply": "2024-08-16T12:58:03.639945Z",
     "shell.execute_reply.started": "2024-08-16T12:57:52.371774Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.sql(query)\n",
    "df = df.sample(fraction=0.05).limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "244b895a-c155-4e77-8af1-71406d0eff17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T12:58:03.641558Z",
     "iopub.status.busy": "2024-08-16T12:58:03.641360Z",
     "iopub.status.idle": "2024-08-16T12:58:10.894965Z",
     "shell.execute_reply": "2024-08-16T12:58:10.894371Z",
     "shell.execute_reply.started": "2024-08-16T12:58:03.641542Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CollectLimit 1000\n",
      "+- *(1) Sample 0.0, 0.05, false, 272233669539652872\n",
      "   +- *(1) Project [p_codmes#180 AS cod_mes#4, key_value#28, ingreso_bruto#34, cod_tipo_documento#27 AS tip_doc#6]\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet e_perm_aws.t_aws_360_cliente_mes_d3[cod_tipo_documento#27,key_value#28,ingreso_bruto#34,p_codmes#180] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3://interbank-datalake-prod-us-east-1-195034049652-stage/pre-stage/da..., PartitionFilters: [isnotnull(p_codmes#180), (cast(p_codmes#180 as int) = 202406)], PushedFilters: [], ReadSchema: struct<cod_tipo_documento:int,key_value:string,ingreso_bruto:decimal(38,10)>"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0025c9d-fcb9-4bcb-bc64-3f1c26bedd2d",
   "metadata": {},
   "source": [
    "### Proceso de análisis en Spark SQL:\n",
    "1. **Análisis de la Sintaxis (Parsing)**: Cuando escribís una query, Spark revisa que esté bien escrita. Si hay un error en la sintaxis o usás una función que no existe (como `date_parse`), Spark te avisa antes de ejecutar la consulta.\n",
    "\n",
    "2. **Análisis de los Elementos (Analysis)**: Spark verifica que las columnas, tablas y funciones que mencionás en la query existan y sean correctas. Si algo no está bien, te mostrará un error antes de seguir.\n",
    "\n",
    "3. **Optimización (Optimization)**: Si todo está bien, Spark reorganiza la consulta para hacerla más eficiente.\n",
    "\n",
    "Entonces, sí, Spark detecta errores como funciones que no existen antes de traer los datos o ejecutar la consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ca616e-99f8-4b4a-b02c-07f9b01990df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-16T12:59:01.686591Z",
     "iopub.status.busy": "2024-08-16T12:59:01.686299Z",
     "iopub.status.idle": "2024-08-16T12:59:01.911659Z",
     "shell.execute_reply": "2024-08-16T12:59:01.911125Z",
     "shell.execute_reply.started": "2024-08-16T12:59:01.686572Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField('cod_mes', StringType(), True)\n",
      "StructField('key_value', StringType(), True)\n",
      "StructField('ingreso_bruto', DecimalType(38,10), True)\n",
      "StructField('tip_doc', IntegerType(), True)"
     ]
    }
   ],
   "source": [
    "# ###########################\n",
    "# Revisar el esquema de datos\n",
    "# ###########################\n",
    "for s in df.schema:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c077165-b373-47e6-b6a3-fb0e54b41caa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-15T19:39:18.996841Z",
     "iopub.status.busy": "2024-08-15T19:39:18.996470Z",
     "iopub.status.idle": "2024-08-15T19:41:07.159359Z",
     "shell.execute_reply": "2024-08-15T19:41:07.158542Z",
     "shell.execute_reply.started": "2024-08-15T19:39:18.996821Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e328889ec24ae9a8f7bf35da808352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "# Accion para traer los datos a memoria // Rompemos la ejecucion perezosa >>> lazy evaluation\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "310dbdf0-f5e6-4961-9bea-d1474ec919d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-15T17:30:30.319180Z",
     "iopub.status.busy": "2024-08-15T17:30:30.318827Z",
     "iopub.status.idle": "2024-08-15T17:30:30.334185Z",
     "shell.execute_reply": "2024-08-15T17:30:30.333514Z",
     "shell.execute_reply.started": "2024-08-15T17:30:30.319128Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def nueva_feature(df):\n",
    "#     df = df.withColumn(\"feature1\", substring(\"key_value\", 1, 8))\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6b1a53-e893-4128-9fcb-ab406cd810ca",
   "metadata": {},
   "source": [
    "### Aplicar esquemas de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed550c39-45d3-489d-b702-3406fa529304",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transformations(df, RECORD_EVENT_TIME_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d035fce-46c3-4e84-bc02-acd071318ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.createOrReplaceTempView(\"table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
